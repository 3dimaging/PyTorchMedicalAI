{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of shlomo_dl_0009_tensor-rt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deeponcology/PyTorchMedicalAI/blob/master/shlomo_dl_0009_tensor_rt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bTPWD8_Bde6e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning in Medical AI 2018/2019 using PyTorch & Google Collab.\n",
        "\n",
        "\n",
        "<table align=\"left\"><td>\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/drive/1JEIeD_445sFvcjSrITB5Z_oW8VHRS_kA\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
        "</td><td>\n",
        "<a target=\"_blank\" href=\"https://github.com/deeponcology/PyTorchMedicalAI/blob/master/shlomo_dl_0001_cuda_collab_pytorch.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>\n",
        "\n",
        "<img src=\"https://github.com/deeponcology/PyTorchMedicalAI/raw/master/assets/tumor_visdom.jpg\" align=\"center\" width=30%>\n",
        "\n",
        "### Author: \n",
        "***Shlomo Kashani***, Head of AI at www.DeepOncology.AI, shlomo@deeponcology.ai \n",
        "\n",
        "\n",
        "### Synopsys:\n",
        "Using TensorRT to extract VGG features using the paper:https://github.com/alexander-rakhlin/ICIAR2018 \n",
        "\n",
        "<img src=\"https://github.com/alexander-rakhlin/ICIAR2018/raw/master/pics/nn_diagram.png\" align=\"center\" width=50%>\n",
        "\n",
        "### DataSets:\n",
        "https://github.com/alexander-rakhlin/ICIAR2018\n"
      ]
    },
    {
      "metadata": {
        "id": "9b43TUE3SWoF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Collab notebook: 009 using TensorRT with Keras and TF"
      ]
    },
    {
      "metadata": {
        "id": "d3aNRnqJafbb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reset -f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JXcBzT_wJtRm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TRT version\n",
        "We need to download this file and access it from Drive\n",
        "- https://developer.nvidia.com/nvidia-tensorrt-4x-download\n",
        "- https://developer.nvidia.com/compute/machine-learning/tensorrt/4.0/ga/TensorRT-4.0.1.6.Ubuntu-16.04.4.x86_64-gnu.cuda-9.2.cudnn7.1"
      ]
    },
    {
      "metadata": {
        "id": "sdg1ZP2oJ_3l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0e56c553-3ebe-4e6e-d63f-b2e2c93deec7"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HzKfosD9KUhz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c48cd590-0c48-44f9-c61a-53d4dd2bd796"
      },
      "cell_type": "code",
      "source": [
        "! pwd\n",
        "! ls -la '/content/drive/My Drive/TRT/'\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "total 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bW70vAQIZrqM",
        "colab_type": "code",
        "outputId": "73a5df01-0327-4cb9-ace3-ffef2674865e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Do we have cuda?!\n",
        "!which nvcc\n",
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/cuda/bin/nvcc\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Tue_Jun_12_23:07:04_CDT_2018\n",
            "Cuda compilation tools, release 9.2, V9.2.148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ex5a3cdKJBDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a20a1919-f87e-4b15-806b-3b7545df94db"
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Jan 31 17:15 .\n",
            "drwxr-xr-x 1 root root 4096 Feb  2 12:16 ..\n",
            "drwxr-xr-x 4 root root 4096 Jan 31 17:14 .config\n",
            "drwxr-xr-x 1 root root 4096 Jan 31 17:15 sample_data\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iaqEb_BbaQIL",
        "colab_type": "code",
        "outputId": "95f5d98b-1264-46ab-ca52-38047ac02ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Let's check if a GPU accelerator card is attached in our machine:\n",
        "!ls -l /dev/nv*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crw-rw-rw- 1 root root 195,   0 Feb  2 11:37 /dev/nvidia0\n",
            "crw-rw-rw- 1 root root 195, 255 Feb  2 11:37 /dev/nvidiactl\n",
            "crw-rw-rw- 1 root root 248,   0 Feb  2 11:37 /dev/nvidia-uvm\n",
            "crw-rw-rw- 1 root root 248,   1 Feb  2 11:37 /dev/nvidia-uvm-tools\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W7lzi2oBcBRl",
        "colab_type": "code",
        "outputId": "3afd031d-bf99-470b-dfd4-159e0cdab2df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# This works too, GPU count and name\n",
        "!nvidia-smi -L"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-95dc9ff4-cd65-3c55-a686-4764167feab4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nav3_MGVcOJw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Use this command to see GPU activity while doing Deep Learning tasks, for this command 'nvidia-smi' and for above one to work, go to 'Runtime > change runtime type > Hardware Accelerator > GPU'"
      ]
    },
    {
      "metadata": {
        "id": "TgKf0GmncYLm",
        "colab_type": "code",
        "outputId": "f27727cf-feed-4658-e853-1d0e646d00f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb  2 11:57:09 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LO7apCNUcluu",
        "colab_type": "code",
        "outputId": "fd7c5362-d767-47e3-cc02-514131d55eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!lscpu |grep 'Model name'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iogQyqtzctNX",
        "colab_type": "code",
        "outputId": "37029c1b-bdd4-406b-df21-73b82ebb793d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#no.of threads each core is having\n",
        "!lscpu | grep 'Thread(s) per core'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread(s) per core:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gjkLboa0czY6",
        "colab_type": "code",
        "outputId": "937eb6ce-e8b4-465a-a3db-72a9c2a21898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#memory that we can use\n",
        "!cat /proc/meminfo | grep 'MemAvailable'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MemAvailable:   12482952 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YqgET6GnczdN",
        "colab_type": "code",
        "outputId": "ef4636b4-69e6-4229-cce9-ff2b10167ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#hard disk that we can use\n",
        "!df -h / | awk '{print $4}'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avail\n",
            "319G\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VysafJ-mczmf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zOQOlxCcZb91",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bTTxigDpa7qG",
        "colab_type": "code",
        "outputId": "9b8d67f7-e9c8-40c1-88a6-fd8e82ef5039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# This magic will create a new CU file\n",
        "# To write a CUDA C program, we need to:\n",
        "\n",
        "#Create a source code fi le with the special fi le name extension of .cu.\n",
        "#Compile the program using the CUDA nvcc compiler.\n",
        "#Run the executable file from the command line, which contains the kernel code executable on the GPU.\n",
        "\n",
        "%%file version.cu\n",
        "#include <thrust/version.h>\n",
        "#include <iostream>\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int major = THRUST_MAJOR_VERSION;\n",
        "  int minor = THRUST_MINOR_VERSION;\n",
        "\n",
        "  std::cout << \"Thrust v\" << major << \".\" << minor << std::endl;\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting version.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e8zG5DLkbIJt",
        "colab_type": "code",
        "outputId": "2aa0a03c-b449-4e14-bfc6-0bb4984f6bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# nvcc is the CUDA compiler \n",
        "!nvcc version.cu -o version\n",
        "!./version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thrust v1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aTy-KdqmpO7g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Memory footprint support libraries/code"
      ]
    },
    {
      "metadata": {
        "id": "JhM0J7WprERD",
        "colab_type": "code",
        "outputId": "831fef16-1a03-4275-832a-249926d71298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  I Proc size: 142.4 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aN2tmUFVoc-j",
        "colab_type": "code",
        "outputId": "e65033b2-338e-4942-ae72-85fe3907694a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.version\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.6.7 (default, Oct 22 2018, 11:32:17) \\n[GCC 8.2.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Ey-bpdSnoi1z",
        "colab_type": "code",
        "outputId": "25309791-707a-4fa0-c201-aec041d560f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# !pip3 install torch==0.4\n",
        "# !pip3 install torchvision\n",
        "\n",
        "!pip3 install 'torch==0.4.0'\n",
        "!pip3 install 'torchvision==0.2.1'\n",
        "!pip3 install --no-cache-dir -I 'pillow==5.1.0'\n",
        "\n",
        "# Restart Kernel\n",
        "# This workaround is needed to properly upgrade PIL on Google Colab.\n",
        "import os\n",
        "#os._exit(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.4.0 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: torchvision==0.2.1 in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (5.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (1.11.0)\n",
            "Collecting pillow==5.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/4b/8b54ab9d37b93998c81b364557dff9f61972c0f650efa0ceaf470b392740/Pillow-5.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 23.8MB/s \n",
            "\u001b[31mfastai 1.0.42 has requirement torch>=1.0.0, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-5.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JAwWysp0qUv8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import PyTorch once again"
      ]
    },
    {
      "metadata": {
        "id": "fSuAKJj2qX2l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from shutil import copyfile\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from PIL import Image\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as func\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "import random \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vQ5re38qpPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's print the versions"
      ]
    },
    {
      "metadata": {
        "id": "LfL1bWLKqiVh",
        "colab_type": "code",
        "outputId": "1e2bba15-b8b2-4a68-a31d-fd15bdac7c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print('__Python VERSION:', sys.version)\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "print('__CUDA VERSION')\n",
        "from subprocess import call\n",
        "# call([\"nvcc\", \"--version\"]) does not work\n",
        "! nvcc --version\n",
        "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "print('__Devices')\n",
        "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
        "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
        "\n",
        "print ('Available devices ', torch.cuda.device_count())\n",
        "print ('Current cuda device ', torch.cuda.current_device())\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# use_cuda = False\n",
        "\n",
        "print(\"USE CUDA=\" + str (use_cuda))\n",
        "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
        "Tensor = FloatTensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__Python VERSION: 3.6.7 (default, Oct 22 2018, 11:32:17) \n",
            "[GCC 8.2.0]\n",
            "__pyTorch VERSION: 0.4.0\n",
            "__CUDA VERSION\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Tue_Jun_12_23:07:04_CDT_2018\n",
            "Cuda compilation tools, release 9.2, V9.2.148\n",
            "__CUDNN VERSION: 7102\n",
            "__Number CUDA Devices: 1\n",
            "__Devices\n",
            "Active CUDA Device: GPU 0\n",
            "Available devices  1\n",
            "Current cuda device  0\n",
            "USE CUDA=True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t-e5W6W6EdQ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "toqSM78JEyCk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TRT download\n",
        "- https://developer.nvidia.com/nvidia-tensorrt-4x-download\n",
        "- https://developer.nvidia.com/compute/machine-learning/tensorrt/4.0/ga/TensorRT-4.0.1.6.Ubuntu-16.04.4.x86_64-gnu.cuda-9.2.cudnn7.1\n"
      ]
    },
    {
      "metadata": {
        "id": "Dybo5NiLE4vW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}